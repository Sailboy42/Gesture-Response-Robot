<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Gesture Response Robot</title>
  <link rel="stylesheet" href="css/styles.css">
</head>
<body>
  <header class="site-header">
    <div class="container">
      <h1 class="logo">Gesture Response Robot — Project Report</h1>
      <button id="menuBtn" class="menu-btn" aria-label="Toggle menu">☰</button>
      <nav id="mainNav" class="main-nav">
        <a href="#report">Report</a>
        <head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Gesture Response Robot</title><link rel="stylesheet" href="css/styles.css"></head>
      <body><header class="site-header"><div class="container"><h1 class="logo">Gesture Response Robot — Project Report</h1><button id="menuBtn" class="menu-btn" aria-label="Toggle menu">☰</button><nav id="mainNav" class="main-nav"><a href="#report">Report</a><a href="#resources">Resources</a><a href="https://github.com/Sailboy42/Gesture-Response-Robot" target="_blank" rel="noopener">Repo</a></nav></div></header><main><section id="report" class="report container"><h2 id="report-proposal">Project report</h2><p>This site is a static project report for the Gesture Response Robot repository. It includes project background, goals and milestones.</p><nav class="toc" aria-label="Table of contents"><strong>Contents</strong><ul><li><a href="#report-proposal">Proposal</a></li><li><a href="#milestones">Milestones</a></li><li><a href="#risks">Risks</a></li><li><a href="#resources">Resources</a></li></ul></nav><section id="proposal"><h3>Final Project Proposal — Gesture Neato</h3><h4>Team</h4><p>Khoi, Tabby, Owen, Bhargavi</p><h4>Main idea</h4><p>Use a Neato that recognizes hand gestures to trigger actions (stop, spin, take photo).</p><h4>Topics</h4><ul><li>Computer vision / ML (MediaPipe, TensorFlow)</li><li>ROS integration for camera/control</li><li>Path planning & obstacle avoidance</li></ul></section><details id="milestones"><summary><strong>Milestones</strong></summary><ol><li><strong>Week 1</strong>: camera feed, basic detection<p>Using OpenCV we created a real-time hand tracking system. The <code>HandTracker</code> class processes webcam video to detect and track a hand using computer vision.</p><ul><li><strong>Skin Detection</strong>: Uses HSV color space filtering to identify skin regions in the video frame.</li><li><strong>Contour Detection</strong>: Finds the largest contour and filters out small noise regions.</li><li><strong>Visualization</strong>: Displays the detected hand contour overlaid on the video feed, along with a debug view showing the skin detection mask.</li></ul><p>The system processes live video frames, applies filters and morphological operations to reduce noise, and draws a green contour around the detected hand. This is a strong foundation for gesture recognition. Next steps: analyze the hand contour to detect fingers and the hand center to classify gestures.</p></li><li>Week 2: gesture training</li><li>Week 3: map gestures to actions</li><li>Final: stable gesture-to-action mapping</li></ol></details><details id="risks"><summary><strong>Risks</strong></summary><ul><li>Camera/recognition reliability</li><li>Scope and integration complexity</li></ul></details></section><section id="resources" class="resources container"><h2>Resources & links</h2><ul><li><a href="https://github.com/Sailboy42/Gesture-Response-Robot" target="_blank" rel="noopener">Repository on GitHub</a></li><li>Use Issues for tracking tasks.</li></ul></section></main><footer class="site-footer"><div class="container">© Gesture Response Robot — built for demos</div></footer><script src="js/main.js"></script></body></html>
